actor_rollout_ref:
  hybrid_engine: True
  model:
    external_lib: null
    override_config: 
      # LoRA配置 - 大幅减少GPU内存使用
      lora_rank: 64        # LoRA的秩，典型值：8, 16, 32, 64
      lora_alpha: 128      # LoRA的alpha参数，通常是rank的2倍
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # Qwen模型的目标模块
    enable_gradient_checkpointing: True
    use_remove_padding: True  # False
  actor:
    strategy: fsdp  # This is for backward-compatibility
    ppo_micro_batch_size_per_gpu: 1  # 匹配主配置，减少内存使用
    use_dynamic_bsz: True # False
    ppo_max_token_len_per_gpu: 32768  # 匹配32K上下文长度
    grad_clip: 1.0
    ppo_epochs: 1
    shuffle: False
    ulysses_sequence_parallel_size: 2 # sp size，匹配2张GPU
    optim:
      lr: 1e-6 # or 5e-6, larger lr with warm up can result in better performance for SFT training.
      lr_warmup_steps_ratio: 0.  # in experimence lr warmup is helpful for chord-mu
      # min_lr_ratio: null   # only useful for warmup with cosine
      warmup_style: constant  # select from constant/cosine
      total_training_steps: -1  # must be override by program
    fsdp_config:
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
      param_offload: False   # 因为有更多GPU资源，可以关闭卸载提高性能
      optimizer_offload: False
      fsdp_size: -1
  ref:
    fsdp_config:
      param_offload: False   # 因为有更多GPU资源，可以关闭卸载
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
    log_prob_micro_batch_size_per_gpu: 1  # 保持最小批处理大小以节省内存
    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
    log_prob_max_token_len_per_gpu: 32768  # 匹配32K上下文长度
    ulysses_sequence_parallel_size: 2 # sp size，匹配2张GPU

trainer:
  balance_batch: True
  # auto: find the last ckpt to resume. If can't find, start from scratch
  resume_mode: auto # or auto or resume_path if
  default_hdfs_dir: null
  remove_previous_ckpt_in_save: False
  del_local_ckpt_after_load: False
  val_before_train: False
