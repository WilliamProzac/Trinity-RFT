actor_rollout_ref:
  hybrid_engine: True
  model:
    external_lib: null
    override_config: 
      # LoRA配置 - 4B模型使用更大的LoRA参数以提高表达能力
      lora_rank: 32       # 降低LoRA秩，显著减少参数量和显存占用
      lora_alpha: 64      # alpha设置为rank的2倍，保持标准比例
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # Qwen模型的目标模块
    enable_gradient_checkpointing: True
    use_remove_padding: True  # False
  actor:
    strategy: fsdp  # This is for backward-compatibility
    ppo_micro_batch_size_per_gpu: 1  # 显存安全优先，确保训练稳定进行
    use_dynamic_bsz: True # False
    ppo_max_token_len_per_gpu: 32768  # 恢复32K上下文，充分利用模型能力
    grad_clip: 1.0
    ppo_epochs: 1
    shuffle: False
    ulysses_sequence_parallel_size: 1 # 关闭序列并行，避免注意力头整除性问题
    optim:
      lr: 3e-6 # or 5e-6, larger lr with warm up can result in better performance for SFT training.
      lr_warmup_steps_ratio: 0.1  # 10%的步骤用于学习率预热，有助于训练稳定性
      # min_lr_ratio: null   # only useful for warmup with cosine
      warmup_style: constant  # select from constant/cosine
      total_training_steps: -1  # must be override by program
    fsdp_config:
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
      param_offload: True    # 启用参数卸载，节省显存（会略微影响速度）
      optimizer_offload: True   # 启用优化器卸载，进一步节省显存
      fsdp_size: -1
  ref:
    fsdp_config:
      param_offload: True    # 启用参考模型参数卸载，节省显存
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
    log_prob_micro_batch_size_per_gpu: 1  # 与actor保持一致，避免OOM
    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
    log_prob_max_token_len_per_gpu: 32768  # 与actor保持一致，32K上下文
    ulysses_sequence_parallel_size: 1 # 关闭序列并行，避免与2张训练GPU不匹配

trainer:
  balance_batch: True
  # auto: find the last ckpt to resume. If can't find, start from scratch
  resume_mode: auto # or auto or resume_path if
  default_hdfs_dir: null
  remove_previous_ckpt_in_save: False
  del_local_ckpt_after_load: False
  val_before_train: False
